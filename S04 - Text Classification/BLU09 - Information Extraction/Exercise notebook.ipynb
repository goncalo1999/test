{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a3dc8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f881c7002cca9b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU09 - Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e125f899",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4efa4dd0f5a58872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import utils\n",
    "\n",
    "cpu_count = int(os.cpu_count()) if os.cpu_count() != None else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a882aedc21b3fc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise notebook you are going to tackle a very real problem: **Detecting fake news!** You'll create a classification workflow to determine if a piece of news is considered 'reliable' or 'unreliable'. You will start by building some basic features, then extract information from the text, go on to build more features, and finally put it all together.\n",
    "\n",
    "The data set we will be using is the [Fake News data set](https://www.kaggle.com/c/fake-news/overview) from Kaggle. Each piece of news is either reliable or trustworthy, '0', or unreliable and possibly fake, '1'. First, let's load the data and see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c4d9f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62fce116d684ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\r\\nAn Iranian woman has been sentenced ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title              author  \\\n",
       "id                                                                          \n",
       "0   House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...       Darrell Lucus   \n",
       "1   FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2                   Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                 text  label  \n",
       "id                                                            \n",
       "0   House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Let...      1  \n",
       "1   Ever get the feeling your life circles the rou...      0  \n",
       "2   Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3   Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4   Print \\r\\nAn Iranian woman has been sentenced ...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/fakenews/train.csv\"\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "df = df[:5000]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40814b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38f5eb775841d955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have 4 columns that are pretty self-explanatory. Let's drop the author column since we only want to practice our text analysis and drop the title as well for simplicity sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2118e6f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4555d789c2c7417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"author\", \"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ec050",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-043e168c017b27d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's also load SpaCy's module with the [merged entities](https://spacy.io/api/pipeline-functions#merge_entities) (which will come in handy later) and stopwords. We insert the merged entities module into the SpaCy pipeline after the NER module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a289d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb489232f7a726b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gugaa\\Desktop\\LDSA\\.venv_s04\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_md' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d931a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed6699286cf9b2f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here we process the news data with SpaCy to use later on. This might take a while depending on your hardware (a break to walk the dog? üê∂)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6313691e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f6782b20750823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [08:10<00:00, 10.19it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[House Dem Aide: We Didn‚Äôt Even See Comey‚Äôs Letter Until Jason Chaffetz Tweeted It By Darrell Lucus on October 30, 2016 Subscribe Jason Chaffetz on the stump in American Fork, Utah ( image courtesy Michael Jolley, available under a Creative Commons-BY license) \n",
       " With apologies to Keith Olbermann, there is no doubt who the Worst Person in The World is this week‚ÄìFBI Director James Comey. But according to a House Democratic aide, it looks like we also know who the second-worst person is as well. It turns out that when Comey sent his now-infamous letter announcing that the FBI was looking into emails that may be related to Hillary Clinton‚Äôs email server, the ranking Democrats on the relevant committees didn‚Äôt hear about it from Comey. They found out via a tweet from one of the Republican committee chairmen. \n",
       " As we now know, Comey notified the Republican chairmen and Democratic ranking members of the House Intelligence, Judiciary, and Oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. Not long after this letter went out, Oversight Committee Chairman Jason Chaffetz set the political world ablaze with this tweet. FBI Dir just informed me, \"The FBI has learned of the existence of emails that appear to be pertinent to the investigation.\" Case reopened \n",
       " ‚Äî Jason Chaffetz (@jasoninthehouse) October 28, 2016 \n",
       " Of course, we now know that this was not the case . Comey was actually saying that it was reviewing the emails in light of ‚Äúan unrelated case‚Äù‚Äìwhich we now know to be Anthony Weiner‚Äôs sexting with a teenager. But apparently such little things as facts didn‚Äôt matter to Chaffetz. The Utah Republican had already vowed to initiate a raft of investigations if Hillary wins‚Äìat least two years‚Äô worth, and possibly an entire term‚Äôs worth of them. Apparently Chaffetz thought the FBI was already doing his work for him‚Äìresulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \n",
       " But according to a senior House Democratic aide, misreading that letter may have been the least of Chaffetz‚Äô sins. That aide told Shareblue that his boss and other Democrats didn‚Äôt even know about Comey‚Äôs letter at the time‚Äìand only found out when they checked Twitter. ‚ÄúDemocratic Ranking Members on the relevant committees didn‚Äôt receive Comey‚Äôs letter until after the Republican Chairmen. In fact, the Democratic Ranking Members didn‚Äô receive it until after the Chairman of the Oversight and Government Reform Committee, Jason Chaffetz, tweeted it out and made it public.‚Äù \n",
       " So let‚Äôs see if we‚Äôve got this right. The FBI director tells Chaffetz and other GOP committee chairmen about a major development in a potentially politically explosive investigation, and neither Chaffetz nor his other colleagues had the courtesy to let their Democratic counterparts know about it. Instead, according to this aide, he made them find out about it on Twitter. \n",
       " There has already been talk on Daily Kos that Comey himself provided advance notice of this letter to Chaffetz and other Republicans, giving them time to turn on the spin machine. That may make for good theater, but there is nothing so far that even suggests this is the case. After all, there is nothing so far that suggests that Comey was anything other than grossly incompetent and tone-deaf. \n",
       " What it does suggest, however, is that Chaffetz is acting in a way that makes Dan Burton and Darrell Issa look like models of responsibility and bipartisanship. He didn‚Äôt even have the decency to notify ranking member Elijah Cummings about something this explosive. If that doesn‚Äôt trample on basic standards of fairness, I don‚Äôt know what does. \n",
       " Granted, it‚Äôs not likely that Chaffetz will have to answer for this. He sits in a ridiculously Republican district anchored in Provo and Orem; it has a Cook Partisan Voting Index of R+25, and gave Mitt Romney a punishing 78 percent of the vote in 2012. Moreover, the Republican House leadership has given its full support to Chaffetz‚Äô planned fishing expedition. But that doesn‚Äôt mean we can‚Äôt turn the hot lights on him. After all, he is a textbook example of what the House has become under Republican control. And he is also the Second Worst Person in the World. About Darrell Lucus \n",
       " Darrell is a 30-something graduate of the University of North Carolina who considers himself a journalist of the old school. An attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right's worst nightmare--a charismatic Christian who is an unapologetic liberal. His desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. You may know him on Daily Kos as Christian Dem in NC . Follow him on Twitter @DarrellLucus or connect with him on Facebook . Click here to buy Darrell a Mello Yello. Connect,\n",
       " Ever get the feeling your life circles the roundabout rather than heads in a straight line toward the intended destination? [Hillary Clinton remains the big woman on campus in leafy, liberal Wellesley, Massachusetts. Everywhere else votes her most likely to don her inauguration dress for the remainder of her days the way Miss Havisham forever wore that wedding dress.  Speaking of Great Expectations, Hillary Rodham overflowed with them 48 years ago when she first addressed a Wellesley graduating class. The president of the college informed those gathered in 1969 that the students needed ‚Äúno debate so far as I could ascertain as to who their spokesman was to be‚Äù (kind of the like the Democratic primaries in 2016 minus the   terms unknown then even at a Seven Sisters school). ‚ÄúI am very glad that Miss Adams made it clear that what I am speaking for today is all of us ‚Äî  the 400 of us,‚Äù Miss Rodham told her classmates. After appointing herself Edger Bergen to the Charlie McCarthys and Mortimer Snerds in attendance, the    bespectacled in granny glasses (awarding her matronly wisdom ‚Äî  or at least John Lennon wisdom) took issue with the previous speaker. Despite becoming the first   to win election to a seat in the U. S. Senate since Reconstruction, Edward Brooke came in for criticism for calling for ‚Äúempathy‚Äù for the goals of protestors as he criticized tactics. Though Clinton in her senior thesis on Saul Alinsky lamented ‚ÄúBlack Power demagogues‚Äù and ‚Äúelitist arrogance and repressive intolerance‚Äù within the New Left, similar words coming out of a Republican necessitated a brief rebuttal. ‚ÄúTrust,‚Äù Rodham ironically observed in 1969, ‚Äúthis is one word that when I asked the class at our rehearsal what it was they wanted me to say for them, everyone came up to me and said ‚ÄòTalk about trust, talk about the lack of trust both for us and the way we feel about others. Talk about the trust bust.‚Äô What can you say about it? What can you say about a feeling that permeates a generation and that perhaps is not even understood by those who are distrusted?‚Äù The ‚Äútrust bust‚Äù certainly busted Clinton‚Äôs 2016 plans. She certainly did not even understand that people distrusted her. After Whitewater, Travelgate, the vast   conspiracy, Benghazi, and the missing emails, Clinton found herself the distrusted voice on Friday. There was a load of compromising on the road to the broadening of her political horizons. And distrust from the American people ‚Äî  Trump edged her 48 percent to 38 percent on the question immediately prior to November‚Äôs election ‚Äî  stood as a major reason for the closing of those horizons. Clinton described her vanquisher and his supporters as embracing a ‚Äúlie,‚Äù a ‚Äúcon,‚Äù ‚Äúalternative facts,‚Äù and ‚Äúa   assault on truth and reason. ‚Äù She failed to explain why the American people chose his lies over her truth. ‚ÄúAs the history majors among you here today know all too well, when people in power invent their own facts and attack those who question them, it can mark the beginning of the end of a free society,‚Äù she offered. ‚ÄúThat is not hyperbole. ‚Äù Like so many people to emerge from the 1960s, Hillary Clinton embarked upon a long, strange trip. From high school Goldwater Girl and Wellesley College Republican president to Democratic politician, Clinton drank in the times and the place that gave her a degree. More significantly, she went from idealist to cynic, as a comparison of her two Wellesley commencement addresses show. Way back when, she lamented that ‚Äúfor too long our leaders have viewed politics as the art of the possible, and the challenge now is to practice politics as the art of making what appears to be impossible possible. ‚Äù Now, as the big woman on campus but the odd woman out of the White House, she wonders how her current station is even possible. ‚ÄúWhy aren‚Äôt I 50 points ahead?‚Äù she asked in September. In May she asks why she isn‚Äôt president. The woman famously dubbed a ‚Äúcongenital liar‚Äù by Bill Safire concludes that lies did her in ‚Äî  theirs, mind you, not hers. Getting stood up on Election Day, like finding yourself the jilted bride on your wedding day, inspires dangerous delusions.,\n",
       " Why the Truth Might Get You Fired October 29, 2016 \n",
       " The tension between intelligence analysts and political policymakers has always been between honest assessments and desired results, with the latter often overwhelming the former, as in the Iraq War, writes Lawrence Davidson. \n",
       " By Lawrence Davidson \n",
       " For those who might wonder why foreign policy makers repeatedly make bad choices, some insight might be drawn from the following analysis. The action here plays out in the United States, but the lessons are probably universal. \n",
       " Back in the early spring of 2003, George W. Bush initiated the invasion of Iraq. One of his key public reasons for doing so was the claim that the country‚Äôs dictator, Saddam Hussein, was on the verge of developing nuclear weapons and was hiding other weapons of mass destruction. The real reason went beyond that charge and included a long-range plan for ‚Äúregime change‚Äù in the Middle East. President George W. Bush and Vice President Dick Cheney receive an Oval Office briefing from CIA Director George Tenet. Also present is Chief of Staff Andy Card (on right). (White House photo) \n",
       " For our purposes, we will concentrate on the belief that Iraq was about to become a hostile nuclear power. Why did President Bush and his close associates accept this scenario so readily? \n",
       " The short answer is Bush wanted, indeed needed, to believe it as a rationale for invading Iraq. At first he had tried to connect Saddam Hussein to the 9/11 attacks on the U.S. Though he never gave up on that stratagem, the lack of evidence made it difficult to rally an American people, already fixated on Afghanistan, to support a war against Baghdad. \n",
       " But the nuclear weapons gambit proved more fruitful, not because there was any hard evidence for the charge, but because supposedly reliable witnesses, in the persons of exiled anti-Saddam Iraqis (many on the U.S. government‚Äôs payroll ), kept telling Bush and his advisers that the nuclear story was true. \n",
       " What we had was a U.S. leadership cadre whose worldview literally demanded a mortally dangerous Iraq, and informants who, in order to precipitate the overthrow of Saddam, were willing to tell the tale of pending atomic weapons. The strong desire to believe the tale of a nuclear Iraq lowered the threshold for proof . Likewise, the repeated assertions by assumed dependable Iraqi sources underpinned a nationwide U.S. campaign generating both fear and war fever. \n",
       " So the U.S. and its allies insisted that the United Nations send in weapons inspectors to scour Iraq for evidence of a nuclear weapons program (as well as chemical and biological weapons). That the inspectors could find no convincing evidence only frustrated the Bush administration and soon forced its hand. \n",
       " On March 19, 2003, Bush launched the invasion of Iraq with the expectation was that, once in occupation of the country, U.S. inspectors would surely find evidence of those nukes (or at least stockpiles of chemical and biological weapons). They did not. Their Iraqi informants had systematically lied to them. \n",
       " Social and Behavioral Sciences to the Rescue? \n",
       " The various U.S. intelligence agencies were thoroughly shaken by this affair, and today, 13 years later, their directors and managers are still trying to sort it out ‚Äì specifically, how to tell when they are getting ‚Äútrue‚Äù intelligence and when they are being lied to. Or, as one intelligence worker has put it, we need ‚Äú help to protect us against armies of snake oil salesmen. ‚Äù To that end the CIA et al. are in the market for academic assistance. Ahmed Chalabi, head of the Iraqi National Congress, a key supplier of Iraqi defectors with bogus stories of hidden WMD. \n",
       " A ‚Äúpartnership‚Äù is being forged between the Office of the Director of National Intelligence (ODNI), which serves as the coordinating center for the sixteen independent U.S. intelligence agencies, and the National Academies of Sciences, Engineering and Medicine . The result of this collaboration will be a ‚Äú permanent Intelligence Community Studies Board‚Äù to coordinate programs in ‚Äúsocial and behavioral science research [that] might strengthen national security .‚Äù \n",
       " Despite this effort, it is almost certain that the ‚Äúsocial and behavioral sciences‚Äù cannot give the spy agencies what they want ‚Äì a way of detecting lies that is better than their present standard procedures of polygraph tests and interrogations. But even if they could, it might well make no difference, because the real problem is not to be found with the liars. It is to be found with the believers. \n",
       " The Believers \n",
       " It is simply not true, as the ODNI leaders seem to assert, that U.S. intelligence agency personnel cannot tell, more often than not, that they are being lied to. This is the case because there are thousands of middle-echelon intelligence workers, desk officers, and specialists who know something closely approaching the truth ‚Äì that is, they know pretty well what is going on in places like Afghanistan, Iraq, Syria, Libya, Israel, Palestine and elsewhere. Director of National Intelligence James Clapper (right) talks with President Barack Obama in the Oval Office, with John Brennan and other national security aides present. (Photo credit: Office of Director of National Intelligence) \n",
       " Therefore, if someone feeds them ‚Äúsnake oil,‚Äù they usually know it. However, having an accurate grasp of things is often to no avail because their superiors ‚Äì those who got their appointments by accepting a pre-structured worldview ‚Äì have different criterion for what is ‚Äútrue‚Äù than do the analysts. \n",
       " Listen to Charles Gaukel, of the National Intelligence Council ‚Äì yet another organization that acts as a meeting ground for the 16 intelligence agencies. Referring to the search for a way to avoid getting taken in by lies, Gaukel has declared, ‚Äú We‚Äôre looking for truth. But we‚Äôre particularly looking for truth that works. ‚Äù Now what might that mean? \n",
       " I can certainly tell you what it means historically. It means that for the power brokers, ‚Äútruth‚Äù must match up, fit with, their worldview ‚Äì their political and ideological precepts. If it does not fit, it does not ‚Äúwork.‚Äù So the intelligence specialists who send their usually accurate assessments up the line to the policy makers often hit a roadblock caused by ‚Äúgroup think,‚Äù ideological blinkers, and a ‚Äúwe know better‚Äù attitude. \n",
       " On the other hand, as long as what you‚Äôre selling the leadership matches up with what they want to believe, you can peddle them anything: imaginary Iraqi nukes, Israel as a Western-style democracy, Saudi Arabia as an indispensable ally, Libya as a liberated country, Bashar al-Assad as the real roadblock to peace in Syria, the Strategic Defense Initiative (SDI) aka Star Wars, a world that is getting colder and not warmer, American exceptionalism in all its glory ‚Äì the list is almost endless. \n",
       " What does this sad tale tell us? If you want to spend millions of dollars on social and behavioral science research to improve the assessment and use of intelligence, forget about the liars. What you want to look for is an antidote to the narrow-mindedness of the believers ‚Äì the policymakers who seem not to be able to rise above the ideological presumptions of their class ‚Äì presumptions that underpin their self-confidence as they lead us all down slippery slopes. \n",
       " It has happened this way so often, and in so many places, that it is the source of Shakespeare‚Äôs determination that ‚Äúwhat is past, is prelude.‚Äù Our elites play out our destinies as if they have no free will ‚Äì no capacity to break with structured ways of seeing. Yet the middle-echelon specialists keep sending their relatively accurate assessments up the ladder of power. Hope springs eternal.]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list(tqdm(nlp.pipe(df[\"text\"], batch_size=20, n_process=cpu_count-1), total=len(df[\"text\"])))\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f722c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-276cdb3161f052ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Overall, the text looks good! Not too many errors, well written... as expected from a news article. Fake news is a very tough, recent problem that is now appearing more and more frequently in the wild. Usually there aren't many ortographic mistakes or slang (as it may happen with spam) since it's coming from news sources that want to appear credible but also clickbaity so that they can profit on that good ad revenue and create distrust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c38a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9871459d26c91a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 - Pipeline\n",
    "\n",
    "Let's create a baseline classification workflow. We'll use the TfidfVectorizer to get a simple, fast and trustworthy baseline.\n",
    "\n",
    "Create a function that applies a pipeline to the given train data, makes a prediction for the test data, and returns the accuracy of the prediction. The pipeline should consist of a `TfidfVectorizer` and a `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b07ff301",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1cf7d1d751604527",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_rf_pipeline(X_train, X_test, y_train, y_test, seed=42):\n",
    "    \"\"\"\n",
    "    Trains a TfidfVectorizer + RandomForestClassifier pipeline on the given train data.\n",
    "    Makes a prediction on the test data.\n",
    "    Returns the trained pipeline and the accuracy of the prediction.\n",
    "\n",
    "    Parameters:\n",
    "        X_train, y_train: train data, pd.Series\n",
    "        X_test, y_test: test data, pd.Series\n",
    "        seed (int): random state seed for the classifier\n",
    "    \n",
    "    Returns:\n",
    "        pipe: fitted pipeline\n",
    "        acc (int): accuracy of the prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    pipe = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('classifier', RandomForestClassifier(random_state=seed))])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac3eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31eb2fa6359bdecb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the baseline, we will preprocess the text - remove punctuaction and stopwords and tokenize it - then run it through the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59999879",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c345c9379c620ae2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.908\n"
     ]
    }
   ],
   "source": [
    "df_processed = df.copy()\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(utils.remove_punctuation)\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(utils.remove_stopwords, stopwords = en_stopwords, \n",
    "                                                  tokenizer = WordPunctTokenizer())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_processed[\"text\"], df_processed[\"label\"], \n",
    "                                                    test_size=0.2, random_state=42, stratify=df_processed[\"label\"])\n",
    "baseline_model, baseline_acc = tfidf_rf_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "assert isinstance(baseline_model, Pipeline)\n",
    "assert hashlib.sha256(json.dumps(str(baseline_model[0])).encode()).hexdigest() == \\\n",
    "'e68c8e581c16f0d62f3b9cb33a7967b17890e18c1fe819d013181e6714e7a303', \"The pipeline parameters are not correct.\"\n",
    "assert hashlib.sha256(json.dumps(str(baseline_model[1])).encode()).hexdigest() == \\\n",
    "'36a4f3295ffa4c170fc0addee2a8cac5613970f06e3dde6956fa31daf19aa329', \"The pipeline parameters are not correct.\"\n",
    "np.testing.assert_almost_equal(baseline_acc, 0.908, decimal=2, err_msg=\"The accuracy is not correct.\")\n",
    "print(f'Baseline accuracy: {baseline_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1ec49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ecc682a6b13cabf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wow, the accuracy is quite good for such a simple text model! This just proves that a trustworthy baseline is all you need. I can't stress enough that it's really important to have a simple first iteration, and afterwards we can add complexity and study which features make sense or not. \n",
    "\n",
    "Sometimes, data scientists focus right off the bat on the most complex solutions and a simple one would be enough. Real life problems will obviously achieve lower scores as the data sets are not controlled or cleaned for you but that should not stop you from starting with a simpler and easier solution.\n",
    "\n",
    "Now let's see if we can engineer more features. We will extract information with SpaCy and see if we can use it to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3f611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9acda58125c99fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2 - SpaCy Matcher\n",
    "\n",
    "Let's see if we can extract some useful features with the SpaCy Matcher.\n",
    "\n",
    "### Exercise 2.1 - Simple matcher\n",
    "\n",
    "You think of some words that could be related with the detection of Fake News. Something starts ringing in your mind about \"propaganda\", \"USA\" and \"fraud\", so you decide to use the SpaCy Matcher to check how many of those words appear in the news articles.\n",
    "\n",
    "Use the `docs` list preprocessed by SpaCy and count the number of occurences of these words in all documents. Make sure to match the words regardless of the case. The output should be the sum of occurencies in all news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f158c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca5f9a618bc32ee6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "words = [\"propaganda\", \"usa\", \"fraud\"]\n",
    "\n",
    "for word in words:\n",
    "    pattern = [[{'LOWER': c.lower()} for c in word.split()]]\n",
    "    matcher.add(word, pattern)\n",
    "count = 0\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id]\n",
    "        span = doc[start:end]\n",
    "        count += 1\n",
    "        #print(f\"Doc {i}, Start: {start}, End: {end}, Match: {span.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6002d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-57893459c2e5a1ac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Not correct, try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hashlib\u001b[38;5;241m.\u001b[39msha256(json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mstr\u001b[39m(count))\u001b[38;5;241m.\u001b[39mencode())\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m \\\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m9d44059c29e077b9fd8496ebcc41c94aeb203bf1adce7729d3ecda30bc885a90\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot correct, try again.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Not correct, try again."
     ]
    }
   ],
   "source": [
    "assert hashlib.sha256(json.dumps(str(count)).encode()).hexdigest() == \\\n",
    "'9d44059c29e077b9fd8496ebcc41c94aeb203bf1adce7729d3ecda30bc885a90', 'Not correct, try again.'\n",
    "print(f'Count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374a2f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9a8e7ce0cab87fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.2 - POS-tagging search\n",
    "\n",
    "Ok, this doesn't look like the way to go, let's look at other theories. You start thinking that fake news might exaggerate on adjectives and adverbs by using over the top descriptions. So you decide to create a feature that counts the number of _Adjectives_ and _Adverbs_ in a piece of news article. The count should be normalized to the token count of the article.\n",
    "\n",
    "The result should be a list of adjective and adverb counts for each document normalized to the token count of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c170ff4c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d048ae4dde4a43cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# nb_adj_adv = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('adjective', [[{'POS': 'ADJ'}]])\n",
    "matcher.add('adverb', [[{'POS': 'ADV'}]])\n",
    "\n",
    "nb_adj_adv = []\n",
    "\n",
    "for doc in docs:\n",
    "    adj_adv_count = sum(1 for token in doc if token.pos_ in ['ADJ', 'ADV'] and not token.is_punct)\n",
    "    word_count = sum(1 for token in doc if not token.is_punct)\n",
    "    normalized_count = adj_adv_count / word_count if word_count > 0 else 0\n",
    "    nb_adj_adv.append(normalized_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4618744a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3961c5c5cb9b18aa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 4 decimals The result is not correct.\n ACTUAL: 0.001454135337364328\n DESIRED: 0.00105",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nb_adj_adv,\u001b[38;5;28mlist\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe result should be a list.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nb_adj_adv) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of the result list is wrong. You should have a count for every news article.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_almost_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_adj_adv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00105\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mThe result is not correct.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_almost_equal(np\u001b[38;5;241m.\u001b[39msum(nb_adj_adv), \u001b[38;5;241m462.5\u001b[39m, decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, err_msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe result is not correct.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2288.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gugaa\\Desktop\\LDSA\\.venv_s04\\Lib\\site-packages\\numpy\\testing\\_private\\utils.py:537\u001b[0m, in \u001b[0;36massert_almost_equal\u001b[1;34m(actual, desired, decimal, err_msg, verbose)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(desired \u001b[38;5;241m-\u001b[39m actual) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64(\u001b[38;5;241m1.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10.0\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39mdecimal)):\n\u001b[1;32m--> 537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(_build_err_msg())\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 4 decimals The result is not correct.\n ACTUAL: 0.001454135337364328\n DESIRED: 0.00105"
     ]
    }
   ],
   "source": [
    "assert isinstance(nb_adj_adv,list), \"The result should be a list.\"\n",
    "assert len(nb_adj_adv) == len(docs), \"The length of the result list is wrong. You should have a count for every news article.\"\n",
    "np.testing.assert_almost_equal(np.var(nb_adj_adv), 0.00105, decimal=4, err_msg='The result is not correct.')\n",
    "np.testing.assert_almost_equal(np.sum(nb_adj_adv), 462.5, decimal=1, err_msg='The result is not correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be4f20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa92a0511c909523",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's add this feature to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa3004",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-820cb2d992a833a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_processed[\"nb_adj_adv\"] = nb_adj_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452de0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54363f2d61151f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.3 - Adjectivized proper nouns\n",
    "\n",
    "Another theory that might be worth testing is that adjectives with proper nouns are often used in this kind of news to induce sentiments towards people or organizations. So you decide to extract proper nouns preceeded by adjectives to maybe use in a later analysis.\n",
    "\n",
    "Create a `Matcher` to search for adjective + proper noun combinations. Count the number of occurences of each combination. Store the 10 most common combinations and the number of their occurences as tuples in a list, sorted in descending order by the number of occurencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a89fe123",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a5547e1de72b597",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('more', 7077),\n",
       " ('also', 5671),\n",
       " ('other', 5301),\n",
       " ('just', 4378),\n",
       " ('so', 4111),\n",
       " ('new', 3988),\n",
       " ('now', 3939),\n",
       " ('most', 3877),\n",
       " ('even', 3869),\n",
       " ('many', 3793)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most_common_adj_propn = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"PROPN\"}]  # Adjective followed by a proper noun\n",
    "matcher.add(\"ADJ_PROPN_PATTERN\", [pattern])\n",
    "\n",
    "# Find matches\n",
    "matches_list = []\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # The matched span\n",
    "        matches_list.append(str(span))\n",
    "\n",
    "# Count occurrences and get the 10 most common\n",
    "matches_counter = Counter(matches_list)\n",
    "most_common_adj_propn = matches_counter.most_common(10)\n",
    "\n",
    "# The result will be a list of tuples (expression, count)\n",
    "most_common_adj_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6de656a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a55276f7a5cb1f58",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The top ten list is not correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(most_common_adj_propn) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt should be the top 10!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(most_common_adj_propn[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mtuple\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe elements of the list should be tuples of (combination, occurences).\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m hashlib\u001b[38;5;241m.\u001b[39msha256(json\u001b[38;5;241m.\u001b[39mdumps(most_common_adj_propn)\u001b[38;5;241m.\u001b[39mencode())\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m \\\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0b12899bfedce520180f460bfd6742c1241ac7270ee98d4dcb482284e134cde8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe top ten list is not correct.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The top ten list is not correct."
     ]
    }
   ],
   "source": [
    "assert isinstance(most_common_adj_propn,list), \"The output should be a list.\"\n",
    "assert len(most_common_adj_propn) == 10, \"It should be the top 10!\"\n",
    "assert isinstance(most_common_adj_propn[0],tuple), 'The elements of the list should be tuples of (combination, occurences).'\n",
    "assert hashlib.sha256(json.dumps(most_common_adj_propn).encode()).hexdigest() == \\\n",
    "'0b12899bfedce520180f460bfd6742c1241ac7270ee98d4dcb482284e134cde8', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79678441",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5de1f6e3a47a8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's look at the 10 most common combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b06dc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a03eaf795b36ce50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_adj_propn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7e98e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7ba7a85acd3ab24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The counts are too low to use these terms as features. Maybe running a vectorizer on all the results could work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f299d11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13de41c333217acd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.4 - Objects of preposition\n",
    "The objects in the sentences could indicate something. For instance, 'NGO financed by Soros' is more likely to appear in fake news than 'NGO financed by UNESCO'. Both objects in these sentences are objects of preposition (hint: SpaCy has a dependency label for this).\n",
    "\n",
    "Create a `Matcher` to search for objects of preposition which are nouns. Again, count the number of occurences of each. Store the 10 most common combinations and their occurences as tuples in a list, sorted in descending order by the number of occurencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ea459a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d047424aca69651a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# most_common_pobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [\n",
    "    {'DEP': 'pobj', 'POS': 'NOUN'}\n",
    "]\n",
    "matcher.add(\"POBJ_NOUN\", [pattern])\n",
    "counter = Counter()\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        pobj = doc[start]\n",
    "        if not pobj.is_punct:\n",
    "            pobj_text = pobj.text\n",
    "            counter[pobj_text] += 1\n",
    "most_common_pobj = counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "938f090a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dd2eee9295599d16",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The top ten list is not correct.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(most_common_pobj) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt should be the top 10!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(most_common_pobj[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mtuple\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe elements of the list should be tuples of (combination, occurences).\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m  hashlib\u001b[38;5;241m.\u001b[39msha256(json\u001b[38;5;241m.\u001b[39mdumps(most_common_pobj)\u001b[38;5;241m.\u001b[39mencode())\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m \\\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8b947c095d53dc4ccc4f28d1a448e60ef2f0c509eeac370cb0448ba9418d25c3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe top ten list is not correct.\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The top ten list is not correct."
     ]
    }
   ],
   "source": [
    "assert isinstance(most_common_pobj,list), \"The output should be a list.\"\n",
    "assert len(most_common_pobj) == 10, \"It should be the top 10!\"\n",
    "assert isinstance(most_common_pobj[0],tuple), 'The elements of the list should be tuples of (combination, occurences).'\n",
    "assert  hashlib.sha256(json.dumps(most_common_pobj).encode()).hexdigest() == \\\n",
    "'8b947c095d53dc4ccc4f28d1a448e60ef2f0c509eeac370cb0448ba9418d25c3', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f44647",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b9f1ddf5f7c7ec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This time the counts are higher and might be more interesting for a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568accf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53fa28fe45c63cfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_pobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98e7c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad6aa5c72ba1a3e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.5 - Verbs with direct objects\n",
    "As the last point, you decide to look at verbs with direct objects. These should indicate actions taken towards something or someone. This exercise can be solved without a Matcher.\n",
    "\n",
    "Search for verbs with direct objects which are not pronouns. This time it's a bit trickier - you need to look at the [parse tree](https://spacy.io/usage/linguistic-features#navigating) because the object does not necessarily come right after the verb. Lemmatize both the verb and the object and count the occurences of the lemmatized verb and direct object separated by a space, like this: 'verb_lemma dobj_lemma'. Don't forget to exclude objects that are pronouns.\n",
    "\n",
    "Again, output the 10 most common combinations and their occurences as tuples in a list, sorted in descending order by the number of occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d62ce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9be3562d21acdf58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# most_common_dobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8593b48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-aa931f62fa91ab35",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(most_common_dobj,list), \"The output should be a list.\"\n",
    "assert len(most_common_dobj) == 10, \"It should be the top 10!\"\n",
    "assert isinstance(most_common_dobj[0],tuple), 'The elements of the list should be tuples of (combination, occurences).'\n",
    "assert hashlib.sha256(json.dumps(most_common_dobj).encode()).hexdigest() == \\\n",
    "'4aa91fbf85175e56f4a132fb253c40869c6f839fc2cfd4cee821ac1422217f29' or \\\n",
    "hashlib.sha256(json.dumps(most_common_dobj).encode()).hexdigest() == \\\n",
    "'7135e8ae4d25d6cf68db7f5083830236e38dae2843b24b6435342d7ded486e45', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85479cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-849a37fbcedc4049",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Not so many occurencies, but again the whole list could be used in a vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b81e2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-218910800564bd8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_dobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244da23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59afa572e05bc308",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3 - Feature unions\n",
    "\n",
    "We're going to create a few more numerical features here, then use them in a feature union pipeline and see if the baseline improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca16469",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692f18d2b61996fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.1 - More features\n",
    "\n",
    "There are a few more simple features that we can extract from the data set to try to enrich our model. Let's add the following features to the `df_processed` dataframe:\n",
    "- number of words in the news article\n",
    "- character length of the news article\n",
    "- average word length\n",
    "- average sentence length.\n",
    "\n",
    "Use the SpaCy processed `Doc`s for calculating the average sentence length (note that you will obtain sentence length in tokens).\n",
    "\n",
    "Use the tokenized text in `df_processed` for everything else. Punctuation and stopwords were already removed from this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb93e53",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6910256a13fa82fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# df_processed[\"nb_words\"] = ...\n",
    "# df_processed[\"doc_length\"] = ...\n",
    "# df_processed[\"avg_word_length\"] = ...\n",
    "# df_processed[\"avg_sentence_length\"] = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5b354",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-31106be84628e3c9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert df_processed.shape == (5000, 7), \"Something wrong about the shape, do you have all columns/rows?\"\n",
    "assert \"nb_words\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"doc_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_word_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_sentence_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "\n",
    "assert np.sum(df_processed[\"nb_words\"]) == 1963935, \"Something is wrong with the nb_words column.\"\n",
    "assert np.sum(df_processed[\"doc_length\"]) == 14636737, \"Something is wrong with the doc_length column.\"\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_word_length\"]), 32100.0, decimal=1, \n",
    "                               err_msg='Something is wrong with the avg_word_length column.')\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_sentence_length\"]), 118628.9, \n",
    "                               decimal=1, err_msg='Something is wrong with the avg_sentence_length column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1e3de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b12d83fd4d036457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.2 - Define a feature union for preprocessing\n",
    "\n",
    "Let's create a processing pipeline for every feature in `df_processed` and join them all in a feature union. The pipeline for textual features should have one step, a `TfidfVectorizer` with default parameters. The pipeline for numerical features should have one step, a `Standard Scaler`. Afterwards, join the features' pipelines in a feature union.\n",
    "\n",
    "Use the `Selector` classes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935c549",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61e270fd37131ec7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from a dataframe \n",
    "    on which to perform additional transformations.\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single text column from the dataframe\n",
    "    on which to perform additional transformations.\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single numerical column from the dataframe\n",
    "    on which to perform additional transformations.\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e0bf1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a5c8a9abba2b5e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# text_pipe = ...\n",
    "# nb_adj_adv_pipe = ...\n",
    "# nb_words_pipe = ...\n",
    "# doc_length_pipe = ...\n",
    "# avg_word_length_pipe = ...\n",
    "# avg_sentence_length_pipe = ...\n",
    "# feats = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bc439",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-98151e411b73b1f5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert len(feats.transformer_list) == 6, \"Did you create a pipeline for each feature?\"\n",
    "for pipe in feats.transformer_list:\n",
    "    \n",
    "    selector = pipe[1][0]\n",
    "    if not (isinstance(selector, TextSelector) or isinstance(selector, NumberSelector)):\n",
    "        raise AssertionError(\"The first step of the pipeline is not correct.\")\n",
    "        \n",
    "    feature_builder = pipe[1][1]\n",
    "    if not (isinstance(feature_builder, TfidfVectorizer) or isinstance(feature_builder, StandardScaler)):\n",
    "        raise AssertionError(\"The second step fo the pipeline is not correct.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb7c8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8c92614d69d640a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.3 Fit the feature union\n",
    "Define a function with pipeline that will apply the preprocessing steps from the previous exercise and fit a classifier to the provided data. The pipeline should have two steps, the feature union from the previous exercise and a `RandomForestClassifier`.\n",
    "The function should fit the pipeline to the train data, make a prediction on the test data and calculate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c958",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb15cc415e6737d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improved_pipeline(feats, X_train, X_test, y_train, y_test, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a pipeline with the provided feature union and a Random Forest classifier.\n",
    "    Fits the pipeline to the train data and makes a prediction with the test data.\n",
    "    Outputs the fitted pipeline and the accuracy of the prediction.\n",
    "\n",
    "    Parameters:\n",
    "        feats: feature union\n",
    "        X_train, y_train: train data\n",
    "        X_test, y_test: test data\n",
    "        seed (int): seed for random state in the classifier\n",
    "\n",
    "    Returns:\n",
    "        pipe: fitted pipeline\n",
    "        acc (int): accuracy of the prediction for the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da0430",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-873e3b1b663a7b12",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Y = df_processed[\"label\"]\n",
    "X = df_processed.drop(columns=\"label\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "pipeline_model, pipeline_acc = improved_pipeline(feats, X_train, X_test, y_train, y_test)\n",
    "\n",
    "assert isinstance(pipeline_model, Pipeline)\n",
    "assert isinstance(pipeline_model[0],FeatureUnion), \"The first step of the pipeline is not correct.\"\n",
    "assert isinstance(pipeline_model[1],RandomForestClassifier),  \"The second step of the pipeline is not correct.\"\n",
    "np.testing.assert_almost_equal(pipeline_acc, 0.908, decimal=3, err_msg=\"The accuracy score is not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e0258",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec9f67cb4f0f5b43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With this more complex approach we have achieved basically the same performance as our baseline. This might mean a lot of things: our features might have no real relevance to the model (which you can check with feature importances) or we have achieved a plateau and can't improve the score with this technique. \n",
    "\n",
    "Nevertheless it is a good score for this problem and data set. Regardless of the score, you have learnt a lot about SpaCy, feature unions and also that the sky is the limit when creating features. Anything can be a feature really - now good features are a totally different thing that might need more research and validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_s04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
